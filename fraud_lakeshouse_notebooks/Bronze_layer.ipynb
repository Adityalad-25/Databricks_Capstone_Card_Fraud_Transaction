{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10af5cad-90f3-418b-a45e-4a16a6e7efeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./01_Project_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08e9b469-19f3-4028-8fcc-c1cba6b15211",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data Ingestion with Auto Loader\n",
    "\n",
    "**`readStream`**\n",
    "Enables real-time data ingestion using Spark Structured Streaming.\n",
    "\n",
    "**`cloudFiles`**\n",
    "The source identifier for **Auto Loader**, optimized for processing massive file volumes in cloud storage.\n",
    "\n",
    "**`cloudFiles.format`**\n",
    "Specifies the input file type (e.g., `csv`).\n",
    "\n",
    "**`cloudFiles.inferColumnTypes`**\n",
    "Triggers an automatic scan to detect data types for each column.\n",
    "\n",
    "**`cloudFiles.schemaLocation`**\n",
    "Persistent storage for the schema; required for tracking evolution and ensuring restart consistency.\n",
    "\n",
    "**`cloudFiles.schemaEvolutionMode`**\n",
    "Set to `addNewColumns` to allow the table to adapt when new fields arrive in source files.\n",
    "\n",
    "**`cloudFiles.rescuedDataColumn`**\n",
    "Captures malformed or schema-mismatched data in a specific column to prevent data loss.\n",
    "\n",
    "**`load`**\n",
    "Starts the read process from the defined landing zone path.\n",
    "\n",
    "**`withColumn`**\n",
    "Creates audit metadata, such as `ingestion_timestamp` and `source_file` path.\n",
    "\n",
    "**`writeStream`**\n",
    "Defines how the processed data is committed to the destination.\n",
    "\n",
    "**`format(\"delta\")`**\n",
    "Writes output in **Delta Lake** format for ACID transactions and versioning.\n",
    "\n",
    "**`outputMode(\"append\")`**\n",
    "Adds new records to the table without overwriting existing data.\n",
    "\n",
    "**`checkpointLocation`**\n",
    "Stores the streaming state (offsets) to ensure the job can resume exactly where it stopped.\n",
    "\n",
    "**`trigger(availableNow=True)`**\n",
    "Executes the stream as a \"micro-batch,\" processing all current data and then shutting down the cluster.\n",
    "\n",
    "**`toTable`**\n",
    "Saves the data as a managed table in Unity Catalog.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61b92601-6915-47de-a0de-612b57b6c80e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "bronze_df = (\n",
    "    spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "        \n",
    "        .option(\"cloudFiles.schemaLocation\", f\"{paths['checkpoints']}/schema/card_txn\")\n",
    "        .option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\")\n",
    "        .option(\"cloudFiles.rescuedDataColumn\", \"_rescued_data\")\n",
    "        .option(\"header\", \"true\")\n",
    "       \n",
    "        .load(paths[\"transactions\"])\n",
    ")\n",
    "\n",
    "\n",
    "bronze_enriched = (\n",
    "    bronze_df\n",
    "        .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "        .withColumn(\"source_file\", col(\"_metadata.file_path\"))\n",
    ")\n",
    "\n",
    "(\n",
    "    bronze_enriched.writeStream\n",
    "        .format(\"delta\")\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"checkpointLocation\", f\"{paths['checkpoints']}/bronze_card_txn\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .trigger(availableNow=True)\n",
    "        # Dynamic table naming based on config\n",
    "        .toTable(f\"{catalog}.{bronze_schema}.card_transactions\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b001f84-6dbf-4fb7-b0d2-5dc764bf37c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT source_file, COUNT(*)\n",
    "FROM fraud_lakehouse.bronze.card_transactions\n",
    "GROUP BY source_file;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69ee72da-39f5-4ce8-9313-620ef04f3180",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4983532343830386,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze_layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
